##### 奥卡姆剃刀（Occam’s razor）原理：

如果一件事情有两种解释，那么最可能正确的解释就是最简单的那个，即假设更少的那个。

这个原理也适用于神经网络学到的模型：给定一些训练数据和一种网络架构，很多组权重值（即很多模型）都可以解释这些数据。简单模型比复杂模型更不容易过拟合。这里的简单模型（simple model）是指参数值分布的熵更小的模型（或参数更少的模型）。

一种常见的降低过拟合的方法就是强制让模型权重只能取较小的值，从而限制模型的复杂度，这使得权重值的分布更加规则（regular）。这种方法叫作权重正则化（weight regularization），其实现方法是向网络损失函数中添加与较大权重值相关的成本（cost）。这个成本有两种形式。

 L1 正则化（L1 regularization）：添加的成本与权重系数的绝对值［权重的 L1 范数（norm）］
成正比。
 L2 正则化（L2 regularization）：添加的成本与权重系数的平方（权重的 L2 范数）成正比。
神经网络的 L2 正则化也叫权重衰减（weight decay）。不要被不同的名称搞混，权重衰减与 L2 正则化在数学上是完全相同的。

